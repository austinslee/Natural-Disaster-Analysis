{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Backend part of NDS portal\n",
        "Used spark for storing dataset and run queries to search and filter data in dataset\n",
        "Used fastAPI to build RestAPI's for the front end to recieve data"
      ],
      "metadata": {
        "id": "BTR0_XNbkuuB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2XGElXecSbvo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e12ffc3-777d-4b47-93ea-43b4c3301aed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.1.tar.gz (281.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.4 MB 40 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[K     |████████████████████████████████| 199 kB 37.0 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.1-py2.py3-none-any.whl size=281845512 sha256=0098cf8af107ee0c7807f26ccfe55f689088456889c0663cd9bdef6776594b0f\n",
            "  Stored in directory: /root/.cache/pip/wheels/43/dc/11/ec201cd671da62fa9c5cc77078235e40722170ceba231d7598\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.1\n"
          ]
        }
      ],
      "source": [
        "# install pyspark package\n",
        "# As we are running python code in google colab we need to upload the dataset file in the runtime files section of colab\n",
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing all necessary packages/libraries"
      ],
      "metadata": {
        "id": "g0D7M96ilu6d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from datetime import datetime, date\n",
        "#import pandas as pd\n",
        "from pyspark.sql import Row\n",
        "import pyspark.pandas as ps"
      ],
      "metadata": {
        "id": "KEpyo2rSS4hf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f927789-dcb6-4a9d-a0b4-a15170139837"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# initializing spark\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "def main():\n",
        "    #first read dataset and clean with pandas\n",
        "    df = ps.read_csv('us_disaster_declarations.csv')\n",
        "    print(df.shape)\n",
        "    df = df[['fema_declaration_string','state','declaration_date','fy_declared','incident_type','declaration_title',\n",
        "    'incident_begin_date','incident_end_date','declaration_request_number','ih_program_declared','ia_program_declared','pa_program_declared',\n",
        "    'hm_program_declared']]\n",
        "    #itemfactors = spark.createDataFrame(model.itemFactors.rdd)\n",
        "    df = df.dropna()\n",
        "    print(df.shape)\n",
        "    spark_df = df.to_spark()\n",
        "    #spark_df.show()\n",
        "    #spark_df.select(\"incident_type\").distinct().show()\n",
        "    #went from 63755 records to 55320 record, 23 to 6 columns\n",
        "    #then convert to spark context after clenaing\n",
        "    spark_df.createOrReplaceTempView('test')\n",
        "    # test if spark is functioning properly by running a query\n",
        "    spark.sql('SELECT * from test where state = \"GA\"').show()\n"
      ],
      "metadata": {
        "id": "ZRBwLpFWS619"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "m5QzPGxmS-M_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2bcda1c-27ee-45e6-9c1c-9fd13e26d297"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `read_csv`, the default index is attached which can cause additional overhead.\n",
            "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(63755, 23)\n",
            "(63755, 13)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `to_spark`, the existing index is lost when converting to Spark DataFrame.\n",
            "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------------+-----+-------------------+-----------+-------------+--------------------+-------------------+--------------------+--------------------------+-------------------+-------------------+-------------------+-------------------+\n",
            "|fema_declaration_string|state|   declaration_date|fy_declared|incident_type|   declaration_title|incident_begin_date|   incident_end_date|declaration_request_number|ih_program_declared|ia_program_declared|pa_program_declared|hm_program_declared|\n",
            "+-----------------------+-----+-------------------+-----------+-------------+--------------------+-------------------+--------------------+--------------------------+-------------------+-------------------+-------------------+-------------------+\n",
            "|                DR-1-GA|   GA|1953-05-02 00:00:00|       1953|      Tornado|             Tornado|1953-05-02 00:00:00|1953-05-02T00:00:00Z|                     53013|                  0|                  1|                  1|                  1|\n",
            "|               DR-16-GA|   GA|1954-03-17 00:00:00|       1954|      Tornado|             Tornado|1954-03-17 00:00:00|1954-03-17T00:00:00Z|                     54003|                  0|                  1|                  1|                  1|\n",
            "|              DR-110-GA|   GA|1961-03-02 00:00:00|       1961|        Flood|              Floods|1961-03-02 00:00:00|1961-03-02T00:00:00Z|                     61011|                  0|                  1|                  1|                  1|\n",
            "|              DR-150-GA|   GA|1963-03-26 00:00:00|       1963|        Flood|Severe Storms & F...|1963-03-26 00:00:00|1963-03-26T00:00:00Z|                     63020|                  0|                  1|                  1|                  1|\n",
            "|              DR-177-GA|   GA|1964-09-10 00:00:00|       1964|    Hurricane|      Hurricane Dora|1964-09-10 00:00:00|1964-09-10T00:00:00Z|                     64024|                  0|                  1|                  1|                  1|\n",
            "|              DR-180-GA|   GA|1964-11-04 00:00:00|       1965|        Flood|            Flooding|1964-11-04 00:00:00|1964-11-04T00:00:00Z|                     64027|                  0|                  1|                  1|                  1|\n",
            "|              DR-214-GA|   GA|1966-03-14 00:00:00|       1966|        Flood|            Flooding|1966-03-14 00:00:00|1966-03-14T00:00:00Z|                     66012|                  0|                  1|                  1|                  0|\n",
            "|              DR-214-GA|   GA|1966-03-14 00:00:00|       1966|        Flood|            Flooding|1966-03-14 00:00:00|1966-03-14T00:00:00Z|                     66012|                  0|                  1|                  1|                  0|\n",
            "|              DR-214-GA|   GA|1966-03-14 00:00:00|       1966|        Flood|            Flooding|1966-03-14 00:00:00|1966-03-14T00:00:00Z|                     66012|                  0|                  1|                  1|                  0|\n",
            "|              DR-214-GA|   GA|1966-03-14 00:00:00|       1966|        Flood|            Flooding|1966-03-14 00:00:00|1966-03-14T00:00:00Z|                     66012|                  0|                  1|                  1|                  0|\n",
            "|              DR-214-GA|   GA|1966-03-14 00:00:00|       1966|        Flood|            Flooding|1966-03-14 00:00:00|1966-03-14T00:00:00Z|                     66012|                  0|                  1|                  1|                  0|\n",
            "|              DR-214-GA|   GA|1966-03-14 00:00:00|       1966|        Flood|            Flooding|1966-03-14 00:00:00|1966-03-14T00:00:00Z|                     66012|                  0|                  1|                  1|                  0|\n",
            "|              DR-214-GA|   GA|1966-03-14 00:00:00|       1966|        Flood|            Flooding|1966-03-14 00:00:00|1966-03-14T00:00:00Z|                     66012|                  0|                  1|                  1|                  0|\n",
            "|              DR-214-GA|   GA|1966-03-14 00:00:00|       1966|        Flood|            Flooding|1966-03-14 00:00:00|1966-03-14T00:00:00Z|                     66012|                  0|                  1|                  1|                  0|\n",
            "|              DR-214-GA|   GA|1966-03-14 00:00:00|       1966|        Flood|            Flooding|1966-03-14 00:00:00|1966-03-14T00:00:00Z|                     66012|                  0|                  1|                  1|                  0|\n",
            "|              DR-214-GA|   GA|1966-03-14 00:00:00|       1966|        Flood|            Flooding|1966-03-14 00:00:00|1966-03-14T00:00:00Z|                     66012|                  0|                  1|                  1|                  0|\n",
            "|              DR-214-GA|   GA|1966-03-14 00:00:00|       1966|        Flood|            Flooding|1966-03-14 00:00:00|1966-03-14T00:00:00Z|                     66012|                  0|                  1|                  1|                  0|\n",
            "|              DR-214-GA|   GA|1966-03-14 00:00:00|       1966|        Flood|            Flooding|1966-03-14 00:00:00|1966-03-14T00:00:00Z|                     66012|                  0|                  1|                  1|                  0|\n",
            "|              DR-214-GA|   GA|1966-03-14 00:00:00|       1966|        Flood|            Flooding|1966-03-14 00:00:00|1966-03-14T00:00:00Z|                     66012|                  0|                  1|                  1|                  0|\n",
            "|              DR-214-GA|   GA|1966-03-14 00:00:00|       1966|        Flood|            Flooding|1966-03-14 00:00:00|1966-03-14T00:00:00Z|                     66012|                  0|                  1|                  1|                  0|\n",
            "+-----------------------+-----+-------------------+-----------+-------------+--------------------+-------------------+--------------------+--------------------------+-------------------+-------------------+-------------------+-------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install Fast API "
      ],
      "metadata": {
        "id": "HBABkUYZmsVB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fastapi"
      ],
      "metadata": {
        "id": "qOn6n-4njqWe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c520ed8-f8b2-4e4e-9b0c-2305b199f29f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fastapi\n",
            "  Downloading fastapi-0.88.0-py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 2.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pydantic!=1.7,!=1.7.1,!=1.7.2,!=1.7.3,!=1.8,!=1.8.1,<2.0.0,>=1.6.2 in /usr/local/lib/python3.8/dist-packages (from fastapi) (1.10.2)\n",
            "Collecting starlette==0.22.0\n",
            "  Downloading starlette-0.22.0-py3-none-any.whl (64 kB)\n",
            "\u001b[K     |████████████████████████████████| 64 kB 3.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.8/dist-packages (from starlette==0.22.0->fastapi) (3.6.2)\n",
            "Requirement already satisfied: typing-extensions>=3.10.0 in /usr/local/lib/python3.8/dist-packages (from starlette==0.22.0->fastapi) (4.1.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.8/dist-packages (from anyio<5,>=3.4.0->starlette==0.22.0->fastapi) (1.3.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.8/dist-packages (from anyio<5,>=3.4.0->starlette==0.22.0->fastapi) (2.10)\n",
            "Installing collected packages: starlette, fastapi\n",
            "Successfully installed fastapi-0.88.0 starlette-0.22.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install Colab Code to run fast api in google colab\n",
        "But after running this library multiple times I noticed that the runtime of colab gets disconnected again and again after sometime. "
      ],
      "metadata": {
        "id": "LSLnrVeJmxDT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install colabcode"
      ],
      "metadata": {
        "id": "Iwe23bwlj8Y6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a1b43e02-d0c0-43c3-c1a8-6280a03da1c7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting colabcode\n",
            "  Downloading colabcode-0.3.0-py3-none-any.whl (5.0 kB)\n",
            "Collecting nest-asyncio==1.4.3\n",
            "  Downloading nest_asyncio-1.4.3-py3-none-any.whl (5.3 kB)\n",
            "Collecting jupyterlab==3.0.7\n",
            "  Downloading jupyterlab-3.0.7-py3-none-any.whl (8.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.3 MB 4.3 MB/s \n",
            "\u001b[?25hCollecting uvicorn==0.13.1\n",
            "  Downloading uvicorn-0.13.1-py3-none-any.whl (45 kB)\n",
            "\u001b[K     |████████████████████████████████| 45 kB 1.1 MB/s \n",
            "\u001b[?25hCollecting pyngrok>=5.0.0\n",
            "  Downloading pyngrok-5.2.1.tar.gz (761 kB)\n",
            "\u001b[K     |████████████████████████████████| 761 kB 30.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: ipython in /usr/local/lib/python3.8/dist-packages (from jupyterlab==3.0.7->colabcode) (7.9.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from jupyterlab==3.0.7->colabcode) (21.3)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.8/dist-packages (from jupyterlab==3.0.7->colabcode) (4.11.2)\n",
            "Collecting nbclassic~=0.2\n",
            "  Downloading nbclassic-0.4.8-py3-none-any.whl (9.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.8 MB 15.6 MB/s \n",
            "\u001b[?25hCollecting tornado>=6.1.0\n",
            "  Downloading tornado-6.2-cp37-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (423 kB)\n",
            "\u001b[K     |████████████████████████████████| 423 kB 61.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: jinja2>=2.10 in /usr/local/lib/python3.8/dist-packages (from jupyterlab==3.0.7->colabcode) (2.11.3)\n",
            "Collecting jupyter-server~=1.2\n",
            "  Downloading jupyter_server-1.23.3-py3-none-any.whl (346 kB)\n",
            "\u001b[K     |████████████████████████████████| 346 kB 42.8 MB/s \n",
            "\u001b[?25hCollecting jupyterlab-server~=2.0\n",
            "  Downloading jupyterlab_server-2.16.3-py3-none-any.whl (54 kB)\n",
            "\u001b[K     |████████████████████████████████| 54 kB 1.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: click==7.* in /usr/local/lib/python3.8/dist-packages (from uvicorn==0.13.1->colabcode) (7.1.2)\n",
            "Collecting h11>=0.8\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 3.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2>=2.10->jupyterlab==3.0.7->colabcode) (2.0.1)\n",
            "Collecting nbconvert>=6.4.4\n",
            "  Downloading nbconvert-7.2.5-py3-none-any.whl (273 kB)\n",
            "\u001b[K     |████████████████████████████████| 273 kB 66.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: nbformat>=5.2.0 in /usr/local/lib/python3.8/dist-packages (from jupyter-server~=1.2->jupyterlab==3.0.7->colabcode) (5.7.0)\n",
            "Requirement already satisfied: traitlets>=5.1 in /usr/local/lib/python3.8/dist-packages (from jupyter-server~=1.2->jupyterlab==3.0.7->colabcode) (5.1.1)\n",
            "Collecting websocket-client\n",
            "  Downloading websocket_client-1.4.2-py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 2.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: prometheus-client in /usr/local/lib/python3.8/dist-packages (from jupyter-server~=1.2->jupyterlab==3.0.7->colabcode) (0.15.0)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.8/dist-packages (from jupyter-server~=1.2->jupyterlab==3.0.7->colabcode) (6.1.12)\n",
            "Collecting argon2-cffi\n",
            "  Downloading argon2_cffi-21.3.0-py3-none-any.whl (14 kB)\n",
            "Collecting anyio<4,>=3.1.0\n",
            "  Downloading anyio-3.6.2-py3-none-any.whl (80 kB)\n",
            "\u001b[K     |████████████████████████████████| 80 kB 7.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.8/dist-packages (from jupyter-server~=1.2->jupyterlab==3.0.7->colabcode) (23.2.1)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.8/dist-packages (from jupyter-server~=1.2->jupyterlab==3.0.7->colabcode) (1.8.0)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.8/dist-packages (from jupyter-server~=1.2->jupyterlab==3.0.7->colabcode) (0.13.3)\n",
            "Collecting sniffio>=1.1\n",
            "  Downloading sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.8/dist-packages (from anyio<4,>=3.1.0->jupyter-server~=1.2->jupyterlab==3.0.7->colabcode) (2.10)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from jupyter-client>=6.1.12->jupyter-server~=1.2->jupyterlab==3.0.7->colabcode) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from jupyterlab-server~=2.0->jupyterlab==3.0.7->colabcode) (2.23.0)\n",
            "Collecting jinja2>=2.10\n",
            "  Downloading Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
            "\u001b[K     |████████████████████████████████| 133 kB 47.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: babel in /usr/local/lib/python3.8/dist-packages (from jupyterlab-server~=2.0->jupyterlab==3.0.7->colabcode) (2.11.0)\n",
            "Collecting json5\n",
            "  Downloading json5-0.9.10-py2.py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: jsonschema>=3.0.1 in /usr/local/lib/python3.8/dist-packages (from jupyterlab-server~=2.0->jupyterlab==3.0.7->colabcode) (4.3.3)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.3 in /usr/local/lib/python3.8/dist-packages (from jupyterlab-server~=2.0->jupyterlab==3.0.7->colabcode) (4.13.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.8.3->jupyterlab-server~=2.0->jupyterlab==3.0.7->colabcode) (3.10.0)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=3.0.1->jupyterlab-server~=2.0->jupyterlab==3.0.7->colabcode) (5.10.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=3.0.1->jupyterlab-server~=2.0->jupyterlab==3.0.7->colabcode) (22.1.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=3.0.1->jupyterlab-server~=2.0->jupyterlab==3.0.7->colabcode) (0.19.2)\n",
            "Collecting nbclassic~=0.2\n",
            "  Downloading nbclassic-0.4.7-py3-none-any.whl (9.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.8 MB 16.2 MB/s \n",
            "\u001b[?25h  Downloading nbclassic-0.4.6-py3-none-any.whl (9.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.8 MB 21.1 MB/s \n",
            "\u001b[?25h  Downloading nbclassic-0.4.5-py3-none-any.whl (9.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.8 MB 33.9 MB/s \n",
            "\u001b[?25h  Downloading nbclassic-0.4.4-py3-none-any.whl (9.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.8 MB 31.2 MB/s \n",
            "\u001b[?25h  Downloading nbclassic-0.4.3-py3-none-any.whl (9.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.7 MB 58.5 MB/s \n",
            "\u001b[?25h  Downloading nbclassic-0.4.2-py3-none-any.whl (9.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.7 MB 40.4 MB/s \n",
            "\u001b[?25h  Downloading nbclassic-0.4.0-py3-none-any.whl (9.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.7 MB 33.3 MB/s \n",
            "\u001b[?25h  Downloading nbclassic-0.3.7-py3-none-any.whl (13 kB)\n",
            "Collecting notebook-shim>=0.1.0\n",
            "  Downloading notebook_shim-0.2.2-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: notebook<7 in /usr/local/lib/python3.8/dist-packages (from nbclassic~=0.2->jupyterlab==3.0.7->colabcode) (5.7.16)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.8/dist-packages (from nbconvert>=6.4.4->jupyter-server~=1.2->jupyterlab==3.0.7->colabcode) (5.0.1)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.8/dist-packages (from nbconvert>=6.4.4->jupyter-server~=1.2->jupyterlab==3.0.7->colabcode) (1.5.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.8/dist-packages (from nbconvert>=6.4.4->jupyter-server~=1.2->jupyterlab==3.0.7->colabcode) (4.6.3)\n",
            "Collecting jupyterlab-pygments\n",
            "  Downloading jupyterlab_pygments-0.2.2-py2.py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: pygments>=2.4.1 in /usr/local/lib/python3.8/dist-packages (from nbconvert>=6.4.4->jupyter-server~=1.2->jupyterlab==3.0.7->colabcode) (2.6.1)\n",
            "Collecting mistune<3,>=2.0.3\n",
            "  Downloading mistune-2.0.4-py2.py3-none-any.whl (24 kB)\n",
            "Collecting nbclient>=0.5.0\n",
            "  Downloading nbclient-0.7.2-py3-none-any.whl (71 kB)\n",
            "\u001b[K     |████████████████████████████████| 71 kB 204 kB/s \n",
            "\u001b[?25hCollecting tinycss2\n",
            "  Downloading tinycss2-1.2.1-py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.8/dist-packages (from nbconvert>=6.4.4->jupyter-server~=1.2->jupyterlab==3.0.7->colabcode) (0.7.1)\n",
            "Collecting jupyter-core\n",
            "  Downloading jupyter_core-5.1.0-py3-none-any.whl (92 kB)\n",
            "\u001b[K     |████████████████████████████████| 92 kB 428 kB/s \n",
            "\u001b[?25hCollecting traitlets>=5.1\n",
            "  Downloading traitlets-5.6.0-py3-none-any.whl (107 kB)\n",
            "\u001b[K     |████████████████████████████████| 107 kB 85.3 MB/s \n",
            "\u001b[?25hCollecting platformdirs>=2.5\n",
            "  Downloading platformdirs-2.5.4-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.8/dist-packages (from nbformat>=5.2.0->jupyter-server~=1.2->jupyterlab==3.0.7->colabcode) (2.16.2)\n",
            "Collecting notebook<7\n",
            "  Downloading notebook-6.5.2-py3-none-any.whl (439 kB)\n",
            "\u001b[K     |████████████████████████████████| 439 kB 43.2 MB/s \n",
            "\u001b[?25h  Downloading notebook-6.5.1-py3-none-any.whl (439 kB)\n",
            "\u001b[K     |████████████████████████████████| 439 kB 86.7 MB/s \n",
            "\u001b[?25h  Downloading notebook-6.4.12-py3-none-any.whl (9.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.9 MB 22.5 MB/s \n",
            "\u001b[?25h  Downloading notebook-6.4.11-py3-none-any.whl (9.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.9 MB 55.6 MB/s \n",
            "\u001b[?25h  Downloading notebook-6.4.10-py3-none-any.whl (9.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.9 MB 50.9 MB/s \n",
            "\u001b[?25h  Downloading notebook-6.4.9-py3-none-any.whl (9.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.9 MB 39.5 MB/s \n",
            "\u001b[?25h  Downloading notebook-6.4.8-py3-none-any.whl (9.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.9 MB 37.6 MB/s \n",
            "\u001b[?25h  Downloading notebook-6.4.7-py3-none-any.whl (9.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.9 MB 21.6 MB/s \n",
            "\u001b[?25h  Downloading notebook-6.4.6-py3-none-any.whl (9.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.9 MB 37.4 MB/s \n",
            "\u001b[?25h  Downloading notebook-6.4.5-py3-none-any.whl (9.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.9 MB 33.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: ipykernel in /usr/local/lib/python3.8/dist-packages (from notebook<7->nbclassic~=0.2->jupyterlab==3.0.7->colabcode) (5.3.4)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.8/dist-packages (from notebook<7->nbclassic~=0.2->jupyterlab==3.0.7->colabcode) (0.2.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.8/dist-packages (from pyngrok>=5.0.0->colabcode) (6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.1->jupyter-client>=6.1.12->jupyter-server~=1.2->jupyterlab==3.0.7->colabcode) (1.15.0)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.8/dist-packages (from terminado>=0.8.3->jupyter-server~=1.2->jupyterlab==3.0.7->colabcode) (0.7.0)\n",
            "Collecting argon2-cffi-bindings\n",
            "  Downloading argon2_cffi_bindings-21.2.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 3.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from argon2-cffi-bindings->argon2-cffi->jupyter-server~=1.2->jupyterlab==3.0.7->colabcode) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.8/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->jupyter-server~=1.2->jupyterlab==3.0.7->colabcode) (2.21)\n",
            "Requirement already satisfied: pytz>=2015.7 in /usr/local/lib/python3.8/dist-packages (from babel->jupyterlab-server~=2.0->jupyterlab==3.0.7->colabcode) (2022.6)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.8/dist-packages (from bleach->nbconvert>=6.4.4->jupyter-server~=1.2->jupyterlab==3.0.7->colabcode) (0.5.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.8/dist-packages (from ipython->jupyterlab==3.0.7->colabcode) (0.7.5)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.8/dist-packages (from ipython->jupyterlab==3.0.7->colabcode) (57.4.0)\n",
            "Collecting jedi>=0.10\n",
            "  Downloading jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 52.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from ipython->jupyterlab==3.0.7->colabcode) (2.0.10)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.8/dist-packages (from ipython->jupyterlab==3.0.7->colabcode) (0.2.0)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.8/dist-packages (from ipython->jupyterlab==3.0.7->colabcode) (4.8.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.8/dist-packages (from ipython->jupyterlab==3.0.7->colabcode) (4.4.2)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.8/dist-packages (from jedi>=0.10->ipython->jupyterlab==3.0.7->colabcode) (0.8.3)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.8/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython->jupyterlab==3.0.7->colabcode) (0.2.5)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->jupyterlab==3.0.7->colabcode) (3.0.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->jupyterlab-server~=2.0->jupyterlab==3.0.7->colabcode) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->jupyterlab-server~=2.0->jupyterlab==3.0.7->colabcode) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->jupyterlab-server~=2.0->jupyterlab==3.0.7->colabcode) (3.0.4)\n",
            "Building wheels for collected packages: pyngrok\n",
            "  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyngrok: filename=pyngrok-5.2.1-py3-none-any.whl size=19792 sha256=f3c28b6930421efa8608f31fee5d1998b57b670f07fd7090cef98d18c7b5713c\n",
            "  Stored in directory: /root/.cache/pip/wheels/5d/f2/70/526da675d32f17577ec47ac4c663084efe39d47c826b6c3bb1\n",
            "Successfully built pyngrok\n",
            "Installing collected packages: traitlets, platformdirs, tornado, jupyter-core, tinycss2, sniffio, nbclient, mistune, jupyterlab-pygments, jinja2, jedi, argon2-cffi-bindings, websocket-client, nbconvert, argon2-cffi, anyio, jupyter-server, notebook-shim, notebook, json5, nbclassic, jupyterlab-server, h11, uvicorn, pyngrok, nest-asyncio, jupyterlab, colabcode\n",
            "  Attempting uninstall: traitlets\n",
            "    Found existing installation: traitlets 5.1.1\n",
            "    Uninstalling traitlets-5.1.1:\n",
            "      Successfully uninstalled traitlets-5.1.1\n",
            "  Attempting uninstall: tornado\n",
            "    Found existing installation: tornado 6.0.4\n",
            "    Uninstalling tornado-6.0.4:\n",
            "      Successfully uninstalled tornado-6.0.4\n",
            "  Attempting uninstall: jupyter-core\n",
            "    Found existing installation: jupyter-core 4.11.2\n",
            "    Uninstalling jupyter-core-4.11.2:\n",
            "      Successfully uninstalled jupyter-core-4.11.2\n",
            "  Attempting uninstall: mistune\n",
            "    Found existing installation: mistune 0.8.4\n",
            "    Uninstalling mistune-0.8.4:\n",
            "      Successfully uninstalled mistune-0.8.4\n",
            "  Attempting uninstall: jinja2\n",
            "    Found existing installation: Jinja2 2.11.3\n",
            "    Uninstalling Jinja2-2.11.3:\n",
            "      Successfully uninstalled Jinja2-2.11.3\n",
            "  Attempting uninstall: nbconvert\n",
            "    Found existing installation: nbconvert 5.6.1\n",
            "    Uninstalling nbconvert-5.6.1:\n",
            "      Successfully uninstalled nbconvert-5.6.1\n",
            "  Attempting uninstall: notebook\n",
            "    Found existing installation: notebook 5.7.16\n",
            "    Uninstalling notebook-5.7.16:\n",
            "      Successfully uninstalled notebook-5.7.16\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires notebook~=5.7.16, but you have notebook 6.4.5 which is incompatible.\n",
            "google-colab 1.0.0 requires tornado~=6.0.4, but you have tornado 6.2 which is incompatible.\n",
            "flask 1.1.4 requires Jinja2<3.0,>=2.10.1, but you have jinja2 3.1.2 which is incompatible.\u001b[0m\n",
            "Successfully installed anyio-3.6.2 argon2-cffi-21.3.0 argon2-cffi-bindings-21.2.0 colabcode-0.3.0 h11-0.14.0 jedi-0.18.2 jinja2-3.1.2 json5-0.9.10 jupyter-core-5.1.0 jupyter-server-1.23.3 jupyterlab-3.0.7 jupyterlab-pygments-0.2.2 jupyterlab-server-2.16.3 mistune-2.0.4 nbclassic-0.3.7 nbclient-0.7.2 nbconvert-7.2.5 nest-asyncio-1.4.3 notebook-6.4.5 notebook-shim-0.2.2 platformdirs-2.5.4 pyngrok-5.2.1 sniffio-1.3.0 tinycss2-1.2.1 tornado-6.2 traitlets-5.6.0 uvicorn-0.13.1 websocket-client-1.4.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tornado"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing all packages"
      ],
      "metadata": {
        "id": "7-dJpY8tnLRu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from fastapi import FastAPI\n",
        "from colabcode import ColabCode\n",
        "from fastapi.middleware.cors import CORSMiddleware"
      ],
      "metadata": {
        "id": "sjdZABIYjxmr"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to execute spark query and convert the result into JSON format\n",
        "def exec_spark_sql_query(query):\n",
        "    json_rdd = spark.sql(query).toJSON()\n",
        "    rdd_list = json_rdd.collect()\n",
        "    output_json = ''\n",
        "    for i, entry in enumerate(rdd_list):\n",
        "        if (i == 0):\n",
        "            output_json = output_json + '['\n",
        "        if (i == len(rdd_list) - 1):\n",
        "            return (output_json + entry + ']')\n",
        "        output_json = output_json + entry + ','\n",
        "    return output_json\n",
        "\n",
        "# test to see if everything works till now\n",
        "#ans = exec_spark_sql_query('SELECT * from test where state = \"GA\"')\n",
        "#print(ans)\n",
        "\n",
        "# configuring the port to run the server for Rest API's\n",
        "cc = ColabCode(port=12000, code=False)\n",
        "app = FastAPI()\n",
        "# have to CORS to fast API to enable browsers to accept data from our API's. \n",
        "# if we don't add CORS then our API's request are blocked by the web browser when running the website\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "# just have defined one rest api and depending on the query we perform different functionality\n",
        "# if we would have more time for this project then i would have seperated the functionality into multiple API's. That would be more easy to\n",
        "# manage and understand in code.\n",
        "@app.get(\"/filter/{query}\")\n",
        "async def read_root(query):\n",
        "  if query == \"all\":\n",
        "    # We defined this API to also return the whole dataset but we tried to execute this on our front-end website by the server took \n",
        "    # a lot of time to respond that is why we removed it from our front-end\n",
        "    print(\"performing all\");\n",
        "    queryResult = exec_spark_sql_query('SELECT * from test')\n",
        "    return {\"data\": queryResult}\n",
        "  # query: CA null null\n",
        "  # splitting the query to return data for different search function\n",
        "  # so basically the query format would be in this format: statename disasterType year\n",
        "  # so when we split it we get the first element as statename, second as disaster type and third as year\n",
        "  allQuery = query.split(\" \")\n",
        "  # depending on the fields provided in query we filter out the data by running query in spark and return data \n",
        "  # for example: if we just got state name in query then we will filter or search dataset on the provided state name\n",
        "  # same example goes for other fields as well\n",
        "  # if we are provided multiple fields in the query then we run search or filter dataset on both/all fields provided.\n",
        "  if(allQuery[0] != \"null\" and allQuery[1] != \"null\" and allQuery[2] != \"null\"):\n",
        "    queryString = \"SELECT * from test where state = '\" + allQuery[0] + \"' AND incident_type = '\" + allQuery[1] + \"' AND fy_declared = '\" + allQuery[2] + \"'\"\n",
        "    return {\"data\": exec_spark_sql_query(queryString)}\n",
        "  elif(allQuery[0] != \"null\" and allQuery[1] != \"null\"):\n",
        "    queryString = \"SELECT * from test where state = '\" + allQuery[0] + \"' AND incident_type = '\" + allQuery[1] + \"'\"\n",
        "    return {\"data\": exec_spark_sql_query(queryString)}\n",
        "  elif(allQuery[0] != \"null\" and allQuery[2] != \"null\"):\n",
        "    queryString = \"SELECT * from test where state = '\" + allQuery[0] + \"' AND fy_declared = '\" + allQuery[2] + \"'\"\n",
        "    return {\"data\": exec_spark_sql_query(queryString)}\n",
        "  elif(allQuery[1] != \"null\" and allQuery[2] != \"null\"):\n",
        "    queryString = \"SELECT * from test where incident_type = '\" + allQuery[1] + \"' AND fy_declared = '\" + allQuery[2] + \"'\"\n",
        "    return {\"data\": exec_spark_sql_query(queryString)}\n",
        "  elif(allQuery[0] != \"null\"):\n",
        "    queryString = \"SELECT * from test where state = '\" + allQuery[0] + \"'\"\n",
        "    return {\"data\": exec_spark_sql_query(queryString)}\n",
        "  elif(allQuery[1] != \"null\"):\n",
        "    queryString = \"SELECT * from test where incident_type = '\" + allQuery[1] + \"'\"\n",
        "    return {\"data\": exec_spark_sql_query(queryString)}\n",
        "  elif(allQuery[2] != \"null\"):\n",
        "    queryString = \"SELECT * from test where fy_declared = '\" + allQuery[2] + \"'\"\n",
        "    return {\"data\": exec_spark_sql_query(queryString)}\n"
      ],
      "metadata": {
        "id": "2GOGce3Ej2wi"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# start the server, so that we can call rest api from front-end\n",
        "# once runned successfully, you would see a console message like: Public URL: NgrokTunnel: \"https://026b-34-82-59-139.ngrok.io\" -> \"http://localhost:12000\"\n",
        "# you can use this kind of url \"https://026b-34-82-59-139.ngrok.io\" to call your Rest API \n",
        "cc.run_app(app=app)"
      ],
      "metadata": {
        "id": "Qz0qsfe_k5Mx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10517166-f479-473a-c83b-99eb21b2cc45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2022-12-02T21:31:10+0000 lvl=warn msg=\"can't bind default web address, trying alternatives\" obj=web addr=127.0.0.1:4040\n",
            "INFO:     Started server process [160]\n",
            "INFO:uvicorn.error:Started server process [160]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:uvicorn.error:Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:uvicorn.error:Application startup complete.\n",
            "INFO:     Uvicorn running on http://127.0.0.1:12000 (Press CTRL+C to quit)\n",
            "INFO:uvicorn.error:Uvicorn running on http://127.0.0.1:12000 (Press CTRL+C to quit)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Public URL: NgrokTunnel: \"https://7f39-34-75-188-237.ngrok.io\" -> \"http://localhost:12000\"\n",
            "INFO:     12.156.141.229:0 - \"GET /filter/null%20Flood%20null HTTP/1.1\" 200 OK\n",
            "INFO:     12.156.141.229:0 - \"GET /filter/CA%20null%20null HTTP/1.1\" 200 OK\n"
          ]
        }
      ]
    }
  ]
}